---
title: "Forecasting"
author: "Cindy Kang"
date: "4/7/2017"
output: html_document
---

Load in data as a time series object
```{r}
ts <- ts(unlist(read.csv("/Users/Joyce/Downloads/dataset.csv", as.is = TRUE)))
```


## Exploratory Data Analysis

```{r}
plot(ts, type = 'l')
```

What do you notice? Are any transformations needed?


```{r}
ts.log <- log(ts)
plot(ts.log, type ='l')
```

Are there any trends in the dataset? How many times should we difference?


```{r}
ts.log.d <- diff(ts.log)
plot(ts.log.d, type = 'l')
```

Is seasonal differencing necessary?


```{r}
acf(ts.log.d, lag.max = 100)$acf
```

```{r}
acf(ts.log.d, lag.max = 300)$acf
```

```{r}
ts.log.dd <- diff(ts.log.d, 52)
plot(ts.log.dd, type = 'l')
```

```{r}
acf(ts.log.dd, lag.max = 200)$acf
pacf(ts.log.dd, lag.max = 100)
```

The PACF doesn't give us much additional information. 

But from the ACF plot, an ARMA(0,1) x (0,1)_52 seems reasonable. Let's try fitting the ARIMA model to the transformed data. We know d = 1, D = 1 from above.

---

## Model Fitting 

```{r}
m1 <- arima(ts.log, order = c(0, 1, 1), seasonal = list(order = c(0, 1, 1), period = 52))
m1
tsdiag(m1)
```

The Ljung-Box test suggests this might not be a good fit.

Let's try other models and compare AIC values.

```{r}
m12 <- arima(ts.log, order = c(0, 1, 1), seasonal = list(order = c(0, 1, 2), period = 52))
m12 # AIC(m12) = -1412.3
m21 <- arima(ts.log, order = c(0, 1, 2), seasonal = list(order = c(0, 1, 1), period = 52))
m21 # AIC(m21) = -1427.5 
m22 <- arima(ts.log, order = c(0, 1, 2), seasonal = list(order = c(0, 1, 2), period = 52))
m22 # AIC(m22) = -1426.4
```

Which model should we choose based on AIC values?

Let's try overfitting for diagnostics. There are four parameters to overfit (p,q,P,Q).
Below are the five models I found plausible but note that I also tried other models such as 
`arima(ts.log, order = c(1, 1, 2), seasonal = list(order = c(0, 1, 1), period = 52))`, etc...

```{r}
m1 <- arima(ts.log, order = c(0, 1, 1), 
            seasonal = list(order = c(0, 1, 1), period = 52)) 
m2 <- arima(ts.log, order = c(0, 1, 2), 
            seasonal = list(order = c(0, 1, 1), period = 52))
m3 <- arima(ts.log, order = c(0, 1, 3), 
            seasonal = list(order = c(0, 1, 1), period = 52))
m4 <- arima(ts.log, order = c(0, 1, 4), 
            seasonal = list(order = c(0, 1, 1), period = 52))
m5 <- arima(ts.log, order = c(0, 1, 5), 
            seasonal = list(order = c(0, 1, 1), period = 52))
```


```{r}
tsdiag(m1)
tsdiag(m2)
tsdiag(m3) 
tsdiag(m4) 
tsdiag(m5) 
```


What do we notice?
-diagnostic plots look better for more complex models
-however, we have to be wary of overfitting

How can we check for this?
-AIC and BIC values


```{r}
AIC(m1)
AIC(m2)
AIC(m3) 
AIC(m4)
AIC(m5) 

BIC(m1)
BIC(m2)
BIC(m3) 
BIC(m4)
BIC(m5)
```


Which model does AIC and BIC suggest is best?




---

## Cross Validation

Another method of model diagnostics is cross-validation. We break up the data into two parts: the train dataset and the test dataset. We fit models to the train dataset, and check whether it's a good model by computing the MSE of predictions for the test dataset. 

We write a function to carry out cross-validation and calculate MSE of predictions for your test dataset. Here, for each model, we find the MSE of prediction for the last five periods of the dataset. 

```{r}
computeCVmse <- function(order.totry, seasorder.totry){

}
```


We can use this function to compute the MSEs for our 5 models.

```{r}
MSE1 <- computeCVmse(c(0, 1, 1), c(0,1,1))
MSE2 <- computeCVmse(c(0, 1, 2), c(0,1,1))
MSE3 <- computeCVmse(c(0, 1, 3), c(0,1,1))
MSE4 <- computeCVmse(c(0, 1, 4), c(0,1,1))
MSE5 <- computeCVmse(c(0, 1, 5), c(0,1,1))

MSE1
MSE2
MSE3
MSE4
MSE5
```

Looking at the last period, which model is best?
Looking instead at periods 1 and 2, which model is best?

We choose model 1 for its simplicity. 

Also, not included here, but you may also want to consider removing outliers before fitting your final model.

---

## Forecasting

```{r}
predictions <- predict(m1, n.ahead = 104)$pred
```

We can plot our predictions to see if they make sense.

```{r}
plot(1:(length(ts) + length(predictions)), c(ts, predictions), type = 'l', col = 1)
points((length(ts) + 1) : (length(ts) + length(predictions)), 
       predictions, type = 'l', col = 2)
```


What did we forget?

```{r}
predictions <- exp(predict(m1, n.ahead = 104)$pred)
plot(1:(length(ts) + length(predictions)), c(ts, predictions), type = 'l', col = 1)
points((length(ts) + 1) : (length(ts) + length(predictions)), predictions, type = 'l', col = 2)
```


---

## Creating the Submission File

```{r}
write.table(predictions,
            sep = ",",
            col.names = FALSE,
            row.names = FALSE,
            file = "Example_Firstname_Lastname_StudentIDNumber.txt")

```

This should save the file to your local directory. As a check to make sure it works, we can read in the file and plot the points to make sure it is what we expected.

```{r}
temp <- read.table("Example_Firstname_Lastname_StudentIDNumber.txt", sep = ",")
plot(as.numeric(unlist(temp)))
```












