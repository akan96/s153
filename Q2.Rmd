---
title: "Q2"
author: "Avery Kan"
date: "9 Apr 2017"
output: 
  html_document:
    fig_height: 3
    fig_width: 5
---
<!-- Don't edit in between this line and the one below -->
```{r include=FALSE}
# Don't delete this chunk if you are using the DataComputing package
library(DataComputing)
library(forecast)
```
*Source file* 
```{r, results='asis', echo=FALSE}
includeSourceDocuments()
```
<!-- Don't edit the material above this line -->
```{r}
ts1 <- read.csv("q1_train.csv", as.is = TRUE)
ts1 <- ts(ts1[,2])
ts2 <- read.csv("q2_train.csv", as.is = TRUE)
ts2 <- ts(ts2[,2])

## Exploratory Data Analysis
plot(ts1, type = "l")
plot(ts2, type = 'l')
```

What do you notice? Are any transformations needed?

```{r}
ts2 = ts2 + abs(min(ts2)+.01)
ts2.ln <- log(ts2)
ts2.sq <- sqrt(ts2)

t = 1: length(ts2)

lm.ts2 <- lm(ts2~t+I(t^2)+I(t^3))
ts2.lm <- ts2 - lm.ts2$fitted.values


ts2.d <- diff(ts2.ln)
acf(ts2.d, lag.max = 100)
pacf(ts2.d, lag.max = 100)

fil <- c(0.5/104, rep(1/104,103),0.5/104)
filter(ts2,fil)
plot(t, ts2, type = "l")
points(t, fil, type = "l", col = "red")
```




Parametric fitting

```{r}
f1 = 1
f2 = 2
f3 = 3
d = 18
v1 = cos(2*pi*f1*t/d)
v2 = sin(2*pi*f1*t/d)
v3 = cos(2*pi*f2*t/d)
v4 = sin(2*pi*f2*t/d)
v5 = cos(2*pi*f3*t/d)
v6 = sin(2*pi*f3*t/d)
lin.mod = lm(ts2 ~ v1 + v2 + v3 + v4 + v5 + v6)
t = 1: length(ts2); tt = t^2 ; t3 = t^3
cub.model = lm(lin.mod$residuals ~ 1 + t + tt + t^3)
plot(t, lin.mod$residuals, type = "o", xlab = "Weekly Time", ylab = "ts2",main = "Trend Fit")
points(t, quad.model$fitted, type = "l", col = "red")
plot(t, ts2, type = "o", xlab = "Weekly Time", ylab = "ts2", main = "Trend and Seasonality")
points(t, cub.model$fitted + lin.mod$fitted, type = "l", col = "red")

```

acf, pacf

```{r}
acf(ts2.d)
pacf(ts2.d)
```



## Model Fitting 

```{r}
ts2.m1 <- arima(ts.log, order = c(0, 1, 1), seasonal = list(order = c(0, 1, 1), period = 52))
m1
tsdiag(m1)
```

The Ljung-Box test suggests this might not be a good fit.

Let's try other models and compare AIC values.

```{r}
m12 <- arima(ts.log, order = c(0, 1, 1), seasonal = list(order = c(0, 1, 2), period = 52))
m12 # AIC(m12) = -1412.3
m21 <- arima(ts.log, order = c(0, 1, 2), seasonal = list(order = c(0, 1, 1), period = 52))
m21 # AIC(m21) = -1427.5 
m22 <- arima(ts.log, order = c(0, 1, 2), seasonal = list(order = c(0, 1, 2), period = 52))
m22 # AIC(m22) = -1426.4
```

Which model should we choose based on AIC values?

Let's try overfitting for diagnostics. There are four parameters to overfit (p,q,P,Q).
Below are the five models I found plausible but note that I also tried other models such as 
`arima(ts.log, order = c(1, 1, 2), seasonal = list(order = c(0, 1, 1), period = 52))`, etc...

```{r}
m1 <- arima(ts.log, order = c(0, 1, 1), 
            seasonal = list(order = c(0, 1, 1), period = 52)) 
m2 <- arima(ts.log, order = c(0, 1, 2), 
            seasonal = list(order = c(0, 1, 1), period = 52))
m3 <- arima(ts.log, order = c(0, 1, 3), 
            seasonal = list(order = c(0, 1, 1), period = 52))
m4 <- arima(ts.log, order = c(0, 1, 4), 
            seasonal = list(order = c(0, 1, 1), period = 52))
m5 <- arima(ts.log, order = c(0, 1, 5), 
            seasonal = list(order = c(0, 1, 1), period = 52))
```


```{r}
tsdiag(m1)
tsdiag(m2)
tsdiag(m3) 
tsdiag(m4) 
tsdiag(m5) 
```


What do we notice?
-diagnostic plots look better for more complex models
-however, we have to be wary of overfitting

How can we check for this?
-AIC and BIC values


```{r}
AIC(m1)
AIC(m2)
AIC(m3) 
AIC(m4)
AIC(m5) 

BIC(m1)
BIC(m2)
BIC(m3) 
BIC(m4)
BIC(m5)
```


Which model does AIC and BIC suggest is best?




---

## Cross Validation

Another method of model diagnostics is cross-validation. We break up the data into two parts: the train dataset and the test dataset. We fit models to the train dataset, and check whether it's a good model by computing the MSE of predictions for the test dataset. 

We write a function to carry out cross-validation and calculate MSE of predictions for your test dataset. Here, for each model, we find the MSE of prediction for the last five periods of the dataset. 

```{r}
computeCVmse <- function(order.totry, seasorder.totry){

}
```


We can use this function to compute the MSEs for our 5 models.

```{r}
MSE1 <- computeCVmse(c(0, 1, 1), c(0,1,1))
MSE2 <- computeCVmse(c(0, 1, 2), c(0,1,1))
MSE3 <- computeCVmse(c(0, 1, 3), c(0,1,1))
MSE4 <- computeCVmse(c(0, 1, 4), c(0,1,1))
MSE5 <- computeCVmse(c(0, 1, 5), c(0,1,1))

MSE1
MSE2
MSE3
MSE4
MSE5
```

Looking at the last period, which model is best?
Looking instead at periods 1 and 2, which model is best?

We choose model 1 for its simplicity. 

Also, not included here, but you may also want to consider removing outliers before fitting your final model.

---

## Forecasting

```{r}
predictions <- predict(m1, n.ahead = 104)$pred
```

We can plot our predictions to see if they make sense.

```{r}
plot(1:(length(ts) + length(predictions)), c(ts, predictions), type = 'l', col = 1)
points((length(ts) + 1) : (length(ts) + length(predictions)), 
       predictions, type = 'l', col = 2)
```


What did we forget?

```{r}
predictions <- exp(predict(m1, n.ahead = 104)$pred)
plot(1:(length(ts) + length(predictions)), c(ts, predictions), type = 'l', col = 1)
points((length(ts) + 1) : (length(ts) + length(predictions)), predictions, type = 'l', col = 2)
```


---

## Creating the Submission File

```{r}
write.table(predictions,
            sep = ",",
            col.names = FALSE,
            row.names = FALSE,
            file = "Example_Firstname_Lastname_StudentIDNumber.txt")

```

This should save the file to your local directory. As a check to make sure it works, we can read in the file and plot the points to make sure it is what we expected.

```{r}
temp <- read.table("Example_Firstname_Lastname_StudentIDNumber.txt", sep = ",")
plot(as.numeric(unlist(temp)))
```


